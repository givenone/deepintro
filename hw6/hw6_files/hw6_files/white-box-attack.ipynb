{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White-box attack exercise\n",
    "\n",
    "In this exercise, you will implement the following white-box attacks.\n",
    "1. Fast Gradient Sign Method (FGSM)\n",
    "2. Projected Gradient Descent (PGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# enter the foldername in your Drive where you have saved the unzipped\n",
    "# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'\n",
    "# folders.\n",
    "# e.g. 'cs231n/assignments/assignment1/cs231n'\n",
    "FOLDERNAME = None\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/data\n",
    "!bash download_data.sh\n",
    "%cd ../models\n",
    "!bash download_models.sh\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.experimental.output_all_intermediates(True)\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from cifar10_input import CIFAR10Data\n",
    "from model import Model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Cifar-10 test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previously loaded data\n",
    "try:\n",
    "   del eval_data\n",
    "   print('Clearing previously loaded data')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Load Cifar-10 test set\n",
    "print('Loading Cifar-10 test dataset')\n",
    "DATA_DIR = './data/cifar-10-batches-py'\n",
    "eval_data = CIFAR10Data(DATA_DIR).eval_data\n",
    "\n",
    "# Print the number of samples in the test set\n",
    "print('The number of the test data: {}'.format(eval_data.n))\n",
    "\n",
    "# Print the first 10 samples\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    sample_image = eval_data.xs[i]\n",
    "    sample_label = eval_data.ys[i]\n",
    "    plt.imshow(sample_image.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[sample_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring a naturally-trained ResNet classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset all graphs and session\n",
    "print('Clearing all graphs')\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a naturally-trained model\n",
    "print('Creating a ResNet model')\n",
    "model = Model(mode='eval')\n",
    "sess = tf.Session()\n",
    "\n",
    "# Restore parameters\n",
    "print('Restoring parameters')\n",
    "NAT_MODEL_DIR = './models/naturally_trained'\n",
    "model_file = tf.train.latest_checkpoint(NAT_MODEL_DIR)\n",
    "\n",
    "var_list = {}\n",
    "with tf.variable_scope('', reuse=True):\n",
    "  for var in tf.train.list_variables(model_file)[1:]:\n",
    "    if 'Momentum' not in var[0]:\n",
    "      var_list[var[0]] = tf.get_variable(name=var[0].replace('BatchNorm', 'batch_normalization'))\n",
    "\n",
    "saver = tf.train.Saver(var_list=var_list)\n",
    "saver.restore(sess, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Before implementing attack methods, we have to evaluate the model for the following reasons.\n",
    "1. To check whether the model is successfuly restored. \n",
    "2. To get samples that are correctly classified. We don't have to attack misclassified samples.\n",
    "\n",
    "Note that the indices of the first 100 samples are stored in a variable named `correct_indices`. You will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, sess, data, indices, attack_method=None):\n",
    "    \"\"\"\n",
    "    Given the data specified by the indices, evaluate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: TensorFlow model\n",
    "        sess: TensorFlow session\n",
    "        data: Cifar-10 test dataset\n",
    "        indices: Indices that specifies the data\n",
    "        attack_method (optional): Instance of attack method, If it is not None, the attack method is applied before\n",
    "        evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        correct_prediction: NumPy array with the same shape as the indices. Given an index, 1 if the corresponding\n",
    "        sample is correctly classifed, 0 otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    correct_predictions = np.zeros([0], np.int32)\n",
    "    \n",
    "    num_images = len(indices)\n",
    "    batch_size = 100\n",
    "    num_batches = int(math.ceil(num_images/batch_size))\n",
    "    \n",
    "    # Run batches\n",
    "    for batch in range(num_batches):\n",
    "        # Construct batch\n",
    "        bstart = batch*batch_size\n",
    "        bend = min(bstart+batch_size, num_images)\n",
    "        image_batch = data.xs[indices[bstart:bend]]\n",
    "        image_batch = np.int32(image_batch)\n",
    "        label_batch = data.ys[indices[bstart:bend]]\n",
    "        # Attack batch\n",
    "        if attack_method is not None:\n",
    "            image_batch = attack_method.perturb(image_batch, label_batch, sess)\n",
    "        # Evaluate batch\n",
    "        feed_dict = {\n",
    "            model.x_input: image_batch, \n",
    "            model.y_input: label_batch\n",
    "        }\n",
    "        correct_prediction = sess.run(model.correct_prediction, feed_dict=feed_dict)\n",
    "        correct_predictions = np.concatenate([correct_predictions, correct_prediction], axis=0)\n",
    "    \n",
    "    return correct_predictions\n",
    "\n",
    "# Evaluate the naturally-trained model on the first 1000 samples in the test dataset\n",
    "indices = np.arange(0, 1000)\n",
    "\n",
    "print('Evaluating naturally-trained model')\n",
    "correct_predictions = evaluate(model, sess, eval_data, indices)\n",
    "accuracy = np.mean(correct_predictions)*100\n",
    "print('Accuracy: {:.1f}%'.format(accuracy))\n",
    "\n",
    "# Select the first 100 samples that are correctly classified.\n",
    "correct_indices = np.where(correct_predictions==1)[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "Now, you will implement Fast Gradient Sign Method under $\\ell_{\\infty}$ constraint, the first method of generating adversarial examples proposed by [Goodfellow et al.](https://arxiv.org/abs/1412.6572). The algorithm is as follows.\n",
    "\n",
    "<center>$x_{adv} = x + \\epsilon \\cdot \\text{sgn}(\\nabla_{x} L(x, y, \\theta))$</center>\n",
    "\n",
    "where $x, y$ are an image and the corresponding label, $L$ is a loss function, and $\\epsilon$ is a maximum perturbation. Usually, Cross-Entropy loss is used for $L$. However, there might be many possible choices for $L$, such as Carlini-Wagner loss (https://arxiv.org/abs/1608.04644)\n",
    "\n",
    "Your code for this section will all be written inside `attacks/fgsm_attack.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First implement Fast Gradient Sign Method.\n",
    "# Open attacks/fgsm_attack.py and follow instructions in the file.\n",
    "\n",
    "from attacks.fgsm_attack import FGSMAttack\n",
    "\n",
    "# Check if your implementation is correct.\n",
    "\n",
    "# Default attack setting\n",
    "epsilon = 8\n",
    "loss_func = 'xent'\n",
    "\n",
    "# Create an instance of FGSMAttack\n",
    "fgsm_attack = FGSMAttack(model, epsilon, loss_func)\n",
    "\n",
    "# Run FGSM attack on a sample\n",
    "sample_image = eval_data.xs[correct_indices[0]]\n",
    "sample_image = np.int32(sample_image) # please convert uint8 to int32\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "sample_label = eval_data.ys[correct_indices[0]]\n",
    "sample_label = np.expand_dims(sample_label, axis=0)\n",
    "sample_adv_image = fgsm_attack.perturb(sample_image, sample_label, sess)\n",
    "sample_adv_label = sess.run(model.predictions, feed_dict={model.x_input: sample_adv_image})\n",
    "\n",
    "# Check if the adversarial image is valid\n",
    "assert np.amax(np.abs(sample_image-sample_adv_image)) <= epsilon\n",
    "assert np.amin(sample_adv_image) >= 0\n",
    "assert np.amax(sample_adv_image) <= 255\n",
    "\n",
    "# Plot the original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image[0, ...].astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title('original image ({})'.format(classes[sample_label[0]]))\n",
    "\n",
    "# Plot the adversarial image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_adv_image[0, ...].astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title('adversarial image ({})'.format(classes[sample_adv_label[0]]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of FGSM with varying $\\epsilon$\n",
    "\n",
    "Now, you will evaluate the performance of FGSM with varying a maximum perturbation $\\epsilon \\in [2, 4, 6, 8, 10]$. In this section, you will use Cross-Entropy loss as $L$. The procedure is as follows.\n",
    "\n",
    "1. Given $\\epsilon$, create an instance of FGSMAttack.\n",
    "2. Evaluate the performance of the attack instance over the samples specified by the variable `correct_indices`.\n",
    "3. Calculate attack success rate, which is defined by\n",
    "<center>$\\text{attack success rate (%)}=\\frac{\\text{# samples that are successfully fooled}}{\\text{# samples}}\\times 100$</center>\n",
    "4. Run 1, 2, and 3 for each $\\epsilon\\in [2, 4, 6, 8, 10]$ and draw a plot of attack success rate against $\\epsilon$.\n",
    "\n",
    "If correctly implemented, the success rate will be 80% or higher on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_func = 'xent'\n",
    "epsilons = [2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    fgsm_attack = FGSMAttack(model, epsilon, loss_func)\n",
    "    correct_predictions = evaluate(model, sess, eval_data, correct_indices, attack_method=fgsm_attack)\n",
    "    attack_success_rate = np.mean(1-correct_predictions)*100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-bo', label='FGSM (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of FGSM with Carlini-Wagner loss\n",
    "\n",
    "In this section, you will evaluate the performance of FGSM using Carlini-Wagner loss. Repeat the procedure in the previous section and compare the results.\n",
    "\n",
    "If correctly implemented, the success rate will be 80% or higher on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_func = 'cw'\n",
    "epsilons = [2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    fgsm_attack = FGSMAttack(model, epsilon, loss_func)\n",
    "    correct_predictions = evaluate(model, sess, eval_data, correct_indices, attack_method=fgsm_attack)\n",
    "    attack_success_rate = np.mean(1-correct_predictions)*100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-ro', label='FGSM (cw loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected Gradient Descent (PGD)\n",
    "\n",
    "Next, you will implement Projected Gradient Descent under $\\ell_{\\infty}$ constraint, which is considered as the strongest white-box attack method proposed by [Madry et al.](https://arxiv.org/abs/1706.06083). The algorithm is as follows.\n",
    "\n",
    "<center>$x^0 = x$</center>\n",
    "<center>$x^{t+1} = \\prod_{B_{\\infty}(x, \\epsilon)} [x^{t} + \\alpha \\cdot \\text{sgn}(\\nabla_{x} L(x^{t}, y, \\theta))]$</center>\n",
    "\n",
    "where $x, y$ are an image and the corresponding label, $L$ is a loss function, $\\alpha$ is a step size, $\\epsilon$ is a maximum perturbation, and $B_{\\infty}(x, \\epsilon)$ is a $\\ell_\\infty$ ball of radius $\\epsilon$ centered at $x$.\n",
    "\n",
    "Your code for this section will all be written inside `attacks/pgd_attack.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement Projected Gradient Descent.\n",
    "# Open attacks/pgd_attack.py and follow instructions in the file.\n",
    "\n",
    "from attacks.pgd_attack import PGDAttack\n",
    "\n",
    "# Check if your implementation is correct.\n",
    "\n",
    "# Default attack setting\n",
    "epsilon = 8\n",
    "step_size = 2\n",
    "num_steps = 20\n",
    "loss_func = 'xent'\n",
    "\n",
    "# Create an instance of FGSMAttack\n",
    "pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, loss_func)\n",
    "\n",
    "# Run PGD attack on a sample\n",
    "sample_image = eval_data.xs[correct_indices[0]]\n",
    "sample_image = np.int32(sample_image) # please convert uint8 to int32\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "sample_label = eval_data.ys[correct_indices[0]]\n",
    "sample_label = np.expand_dims(sample_label, axis=0)\n",
    "sample_adv_image = pgd_attack.perturb(sample_image, sample_label, sess)\n",
    "sample_adv_label = sess.run(model.predictions, feed_dict={model.x_input: sample_adv_image})\n",
    "\n",
    "# Check if the adversarial image is valid\n",
    "assert np.amax(np.abs(sample_image-sample_adv_image)) <= epsilon\n",
    "assert np.amin(sample_adv_image) >= 0\n",
    "assert np.amax(sample_adv_image) <= 255\n",
    "\n",
    "# Plot the original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image[0, ...].astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title('original image ({})'.format(classes[sample_label[0]]))\n",
    "\n",
    "# Plot the adversarial image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_adv_image[0, ...].astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title('adversarial image ({})'.format(classes[sample_adv_label[0]]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of PGD with varying $\\epsilon$\n",
    "\n",
    "Now, you will evaluate the performance of PGD with varying maximum perturbation $\\epsilon \\in [2, 4, 6, 8, 10]$. In this section, you will use Cross-Entropy loss as $L$. Step size and the number of iterations are set to 2 and 20 respectively. The procedure is as follows.\n",
    "\n",
    "1. First, create an instance of PGDAttack with given $\\epsilon$.\n",
    "2. Evaluate the performance of the attack instance over the samples specified by the variable `correct_indices`.\n",
    "3. Calculate attack success rate, which is defined by\n",
    "<center>$\\text{attack success rate (%)}=\\frac{\\text{# samples that are successfully fooled}}{\\text{# samples}}\\times 100$</center>\n",
    "4. Run 1, 2, and 3 for each $\\epsilon\\in [2, 4, 6, 8, 10]$ and draw a plot of attack success rate against $\\epsilon$.\n",
    "\n",
    "If correctly implemented, the success rate will be 100% on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_size = 2\n",
    "num_steps = 20\n",
    "loss_func = 'xent'\n",
    "epsilons = [2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, loss_func)\n",
    "    correct_predictions = evaluate(model, sess, eval_data, correct_indices, attack_method=pgd_attack)\n",
    "    attack_success_rate = np.mean(1-correct_predictions)*100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-bo', label='PGD (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of PGD with Carlini-Wagner loss\n",
    "\n",
    "In this section, you will evaluate the performance of PGD using Carlini-Wagner loss. Repeat the procedure in the previous section and compare the results.\n",
    "\n",
    "If correctly implemented, the success rate will be 100% on epsilon 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_size = 2\n",
    "num_steps = 20\n",
    "loss_func = 'cw'\n",
    "epsilons = [2, 4, 6, 8, 10]\n",
    "attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, loss_func)\n",
    "    correct_predictions = evaluate(model, sess, eval_data, correct_indices, attack_method=pgd_attack)\n",
    "    attack_success_rate = np.mean(1-correct_predictions)*100\n",
    "    attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, attack_success_rates, '-ro', label='PGD (cw loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question**\n",
    "\n",
    "Which is better, Cross-Entropy loss or Carlini-Wagner loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacks on adversarially-trained model\n",
    "\n",
    "As you can see, naturally-trained neural networks are vulnerable to adversarial attacks. There are several ways to improve adversarial robustness of neural networks. One example is adversarial training, which uses adversarial samples to train a neural network. It constitutes the current state-of-the-art in the adversarial robustness.\n",
    "\n",
    "PGD adversarial training, proposed by [Madry et al.](https://arxiv.org/abs/1706.06083), utilizes Projected Gradient Descent to train a network. It has been shown that PGD adversarial training on MNIST and Cifar-10 can defend white-box attack successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all graphs and session\n",
    "print('Clearing all graphs')\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a naturally-trained model\n",
    "print('Creating a ResNet model')\n",
    "model = Model(mode='eval')\n",
    "sess = tf.Session()\n",
    "\n",
    "# Restore parameters\n",
    "print('Restoring parameters')\n",
    "ADV_MODEL_DIR = './models/adv_trained'\n",
    "model_file = tf.train.latest_checkpoint(ADV_MODEL_DIR)\n",
    "\n",
    "var_list = {}\n",
    "with tf.variable_scope('', reuse=True):\n",
    "  for var in tf.train.list_variables(model_file)[1:]:\n",
    "    if 'Momentum' not in var[0]:\n",
    "      var_list[var[0]] = tf.get_variable(name=var[0].replace('BatchNorm', 'batch_normalization'))\n",
    "\n",
    "saver = tf.train.Saver(var_list=var_list)\n",
    "saver.restore(sess, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Before implementing attack methods, we have to evaluate the model for the following reasons.\n",
    "1. To check whether the model is successfuly restored. \n",
    "2. To get samples that are correctly classified. We don't have to attack misclassified samples.\n",
    "\n",
    "Note that the indices of the first 100 samples are stored in a variable named `correct_indices`. You will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the adversarially-trained model on the first 1000 samples in the test dataset\n",
    "indices = np.arange(0, 1000)\n",
    "\n",
    "print('Evaluating adversarially-trained model')\n",
    "correct_predictions = evaluate(model, sess, eval_data, indices)\n",
    "accuracy = np.mean(correct_predictions)*100\n",
    "print('Accuracy: {:.1f}%'.format(accuracy))\n",
    "\n",
    "# Select the first 100 samples that are correctly classified.\n",
    "correct_indices = np.where(correct_predictions==1)[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question**\n",
    "\n",
    "Is the accuracy of adversarially-trained model higher than that of naturally-trained model, or lower? Explain why they are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful material**\n",
    "\n",
    "For those who are curious about this phenomenon, see https://arxiv.org/abs/1805.12152."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of FGSM on adversarially-trained model\n",
    "\n",
    "Now, we will evaluate the the performance of FGSM on adversarially-trained model. In this section, you will use Cross-Entropy loss as $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'xent'\n",
    "epsilons = [2, 4, 6, 8, 10]\n",
    "fgsm_attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    fgsm_attack = FGSMAttack(model, epsilon, loss_func)\n",
    "    correct_predictions = evaluate(model, sess, eval_data, correct_indices, attack_method=fgsm_attack)\n",
    "    attack_success_rate = np.mean(1-correct_predictions)*100\n",
    "    fgsm_attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, fgsm_attack_success_rates, '-bo', label='FGSM (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of PGD on adversarially-trained model\n",
    "\n",
    "Now, we will evaluate the the performance of PGD on adversarially-trained model. In this section, you will use Cross-Entropy loss as $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 2\n",
    "num_steps = 20\n",
    "loss_func = 'xent'\n",
    "epsilons = [2, 4, 6, 8, 10]\n",
    "pgd_attack_success_rates = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    pgd_attack = PGDAttack(model, epsilon, step_size, num_steps, loss_func)\n",
    "    correct_predictions = evaluate(model, sess, eval_data, correct_indices, attack_method=pgd_attack)\n",
    "    attack_success_rate = np.mean(1-correct_predictions)*100\n",
    "    pgd_attack_success_rates.append(attack_success_rate)\n",
    "    print('Epsilon: {}, Attack success rate: {:.1f}%'.format(epsilon, attack_success_rate))\n",
    "\n",
    "plt.plot(epsilons, pgd_attack_success_rates, '-ro', label='PGD (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the performance of FGSM and PGD\n",
    "\n",
    "Finally, we compare the performace of FGSM and PGD on adversarially-trained model. Just overlay the plots drawn in the two previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [2, 4, 6, 8, 10]\n",
    "\n",
    "plt.plot(epsilons, fgsm_attack_success_rates, '-bo', label='FGSM (xent loss)')\n",
    "plt.plot(epsilons, pgd_attack_success_rates, '-ro', label='PGD (xent loss)')\n",
    "plt.ylim(-5, 105)\n",
    "plt.xticks(epsilons)\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('attack success rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline question**\n",
    "\n",
    "Describe the result above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**\n",
    "\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
