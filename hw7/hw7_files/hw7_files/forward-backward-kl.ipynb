{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"forward-backward-kl.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"In6NgJhHSMAv","colab_type":"text"},"source":["## KL divergence minimization\n","\n","In this exercise, we will minimize KL divergence between the true distribution $p(x)$ and our model $q_\\theta(x)$ using gradient descent. We will be utilizing [Tensorflow Probability (TFP)](https://www.tensorflow.org/probability) built on Tensorflow that makes it easy to combine probabilistic models and deep learning. If you haven't already, please go ahead and [install](https://www.tensorflow.org/probability/install) the library.\n"]},{"cell_type":"code","metadata":{"id":"svoFffCvSMAw","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow.compat.v1 as tf\n","tf.disable_eager_execution()\n","import tensorflow_probability as tfp\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import subprocess\n","\n","FIGSIZE = (6, 6)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKfM1VkfSMAz","colab_type":"code","colab":{}},"source":["# Evaluate the probabilities given the samples\n","def prob_map(p, lim, N):\n","    x = np.linspace(lim[0], lim[1], N)\n","    y = np.linspace(lim[2], lim[3], N)\n","    xy = np.array(np.meshgrid(x, y)).reshape(2, N**2).T\n","    xy_prob = p.prob(xy.astype(np.float32)).eval()\n","    x, y = np.meshgrid(x, y)\n","    return x, y, xy_prob.reshape((N, N))\n","\n","# Create mean and covariance params for bivariate normal distribution\n","def gen_gaussian_params(d=2):\n","    p_mean = np.random.standard_normal((d,)).astype(np.float32)\n","    p_cov = np.random.standard_normal((d, d)).astype(np.float32)\n","    p_cov[0, 1] = 0\n","    p_cov = np.dot(p_cov, p_cov.T)\n","    return p_mean, p_cov\n","\n","# Plot helpers\n","def contour(x, y, xy_prob, lim, cmap=plt.cm.inferno, axis=None):\n","    if axis is None:\n","        plt.contour(x, y, xy_prob, np.linspace(0., 1.1*xy_prob.max(), 10),\n","            zorder=2, extent=lim, linewidths=4, colors='w', alpha=0.5)\n","        plt.contour(x, y, xy_prob, np.linspace(0., 1.1*xy_prob.max(), 10),\n","                zorder=3, extent=lim, linewidths=2, cmap=cmap)\n","    else:\n","        axis.contour(x, y, xy_prob, np.linspace(0., 1.1*xy_prob.max(), 10),\n","            zorder=2, extent=lim, linewidths=4, colors='w', alpha=0.5)\n","        axis.contour(x, y, xy_prob, np.linspace(0., 1.1*xy_prob.max(), 10),\n","                zorder=3, extent=lim, linewidths=2, cmap=cmap)\n","    \n","def scatter(sample, axis=None):\n","    if axis is None:\n","        plt.scatter(sample[:200, 0], sample[:200, 1], s=25, alpha=0.7, c='k')\n","    else:\n","        axis.scatter(sample[:200, 0], sample[:200, 1], s=25, alpha=0.7, c='k')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gFJoqw9SMA1","colab_type":"text"},"source":["### Create a bivariate normal as the true distribution $p(x) = \\mathcal{N}(\\mu, \\Sigma)$"]},{"cell_type":"markdown","metadata":{"scrolled":false,"id":"7hwMTP5vSMA1","colab_type":"text"},"source":["Draw the probability contour plot for the true distribution p(x)"]},{"cell_type":"code","metadata":{"id":"kB9gaDKVSMA2","colab_type":"code","colab":{}},"source":["np.random.seed(555)\n","tf.reset_default_graph()\n","sess = tf.InteractiveSession()\n","\n","p_mean, p_cov = gen_gaussian_params()\n","p_mean += 0.8\n","p = tfp.distributions.MultivariateNormalFullCovariance(p_mean, p_cov)\n","\n","plt.figure(figsize=FIGSIZE)\n","N = 128\n","lim = [-2.5, 3.5, -2.5, 3.5]\n","x, y, xy_prob = prob_map(p, lim, N)\n","contour(x, y, xy_prob, lim, cmap=plt.cm.viridis)\n","\n","plt.axis(lim)\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dq7vlH79SMA4","colab_type":"text"},"source":["Draw samples from the true distribution $p(x)$ and visualize"]},{"cell_type":"code","metadata":{"id":"vBeRjAaeSMA4","colab_type":"code","colab":{}},"source":["n_sample = 1000\n","sample = p.sample(n_sample).eval()\n","\n","plt.figure(figsize=FIGSIZE)\n","scatter(sample)\n","plt.axis(lim)\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lzlb5Qc1SMA6","colab_type":"text"},"source":["## Part A. Forward KL minimization"]},{"cell_type":"markdown","metadata":{"id":"r-kSdlpsSMA7","colab_type":"text"},"source":["### Create the model distribution \n","Now we will create $q_\\theta(x)$ and apply gradient descent to minimize the forward KL divergence\n","$$\\theta^* = \\arg\\min_\\theta D_{KL}(p(x) \\parallel q_\\theta(x))$$"]},{"cell_type":"code","metadata":{"id":"p2F85XimSMA7","colab_type":"code","colab":{}},"source":["# Define your model here!\n","q_mean = None\n","q_cov = None\n","q = tfp.distributions.MultivariateNormalFullCovariance(q_mean, q_cov)\n","\n","# Define the loss here!\n","loss = None\n","train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n","sess.run(tf.global_variables_initializer())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BaXmEm1USMA9","colab_type":"text"},"source":["Lets run it for 20 iterations and visualize $q_{\\theta^{(t)}}(x)$ "]},{"cell_type":"code","metadata":{"scrolled":false,"id":"JtPEk0YlSMA-","colab_type":"code","colab":{}},"source":["fig, axes = plt.subplots(5,4, figsize=[12, 15])\n","\n","for i in range(5):\n","    for j in range(4):\n","        scatter(sample, axis=axes[i][j])\n","        x, y, xy_prob = prob_map(q, lim, N)\n","        contour(x, y, xy_prob, lim, axis=axes[i][j])\n","        axes[i][j].axis(lim)\n","        axes[i][j].axis('off')\n","        axes[i][j].set_title('iter: {}'.format(i*5 + j))\n","        sess.run(train)\n","        \n","plt.show()\n","plt.close(fig)\n","\n","sess.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cTWopax_SMBA","colab_type":"text"},"source":["### Create a mixture of two bivariate normals as the true distribution\n","$p(x) = \\pi_1 \\mathcal{N}(\\mu_1, \\Sigma_1) + \\pi_2 \\mathcal{N}(\\mu_2, \\Sigma_2)$ where $\\pi_1 + \\pi_2 = 1$"]},{"cell_type":"code","metadata":{"id":"cTCJVJNySMBA","colab_type":"code","colab":{}},"source":["def build_p(mix=0.8):\n","    return tfp.distributions.Mixture(\n","        cat=tfp.distributions.Categorical(probs=[mix, 1.-mix]),\n","        components=[\n","            tfp.distributions.MultivariateNormalFullCovariance(\n","                loc=[0.5, -0.7], covariance_matrix=[[1., 1.], [1., 2.]]),\n","            tfp.distributions.MultivariateNormalFullCovariance(\n","                loc=[-2.5, 2.5], covariance_matrix=[[0.5, 0.1], [0.1, 0.5]]),\n","        ])\n","\n","np.random.seed(8)\n","tf.reset_default_graph()\n","sess = tf.InteractiveSession()\n","\n","p = build_p()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7ywmHHSSMBC","colab_type":"text"},"source":["Draw the probability contour plot for the true distribution p(x)"]},{"cell_type":"code","metadata":{"id":"kQoQezxISMBD","colab_type":"code","colab":{}},"source":["N = 128\n","lim = [-4, 4, -4, 4]\n","plt.figure(figsize=FIGSIZE)\n","x, y, xy_prob = prob_map(p, lim, N)\n","contour(x, y, xy_prob, lim, cmap=plt.cm.viridis)\n","plt.axis(lim)\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCipP6uYSMBE","colab_type":"text"},"source":["Draw samples from the true distribution $p(x)$ and visualize"]},{"cell_type":"code","metadata":{"id":"YbY33vB4SMBF","colab_type":"code","colab":{}},"source":["n_sample = 1000\n","sample = p.sample(n_sample).eval()\n","plt.figure(figsize=FIGSIZE)\n","scatter(sample)\n","plt.axis(lim)\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNE0x6WsSMBH","colab_type":"text"},"source":["### Create the model distribution \n","Now we will create $q_\\theta(x)$ and apply gradient descent to minimize the forward KL divergence\n","$$\\theta^* = \\arg\\min_\\theta D_{KL}(p(x) \\parallel q_\\theta(x))$$"]},{"cell_type":"code","metadata":{"id":"E-wEw-u9SMBH","colab_type":"code","colab":{}},"source":["# Define your model here!\n","q_mean = None\n","q_cov = None\n","q = tfp.distributions.MultivariateNormalFullCovariance(q_mean, q_cov)\n","\n","# Define the loss here!\n","logp = None\n","train = tf.train.GradientDescentOptimizer(0.1).minimize(None)\n","sess.run(tf.global_variables_initializer())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OE-IUq6DSMBK","colab_type":"text"},"source":["Lets run it for 20 iterations and visualize $q_{\\theta^{(t)}}(x)$ "]},{"cell_type":"code","metadata":{"id":"2VBxfiY0SMBK","colab_type":"code","colab":{}},"source":["fig, axes = plt.subplots(5,4, figsize=[12, 15])\n","\n","for i in range(5):\n","    for j in range(4):\n","        scatter(sample, axis=axes[i][j])\n","        x, y, xy_prob = prob_map(q, lim, N)\n","        contour(x, y, xy_prob, lim, axis=axes[i][j])\n","        axes[i][j].axis(lim)\n","        axes[i][j].axis('off')\n","        axes[i][j].set_title('iter: {}'.format(i*5 + j))\n","        sess.run(train)\n","        \n","plt.show()\n","plt.close(fig)\n","\n","sess.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQJgLIASSMBM","colab_type":"text"},"source":["## Part B. Reverse KL minimization"]},{"cell_type":"code","metadata":{"id":"3BOGx7ZiSMBM","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()\n","sess = tf.InteractiveSession()\n","p = build_p()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LcDkl7a1SMBO","colab_type":"text"},"source":["### Create the model distribution \n","Now we will create $q_\\theta(x)$ and apply gradient descent to minimize the reverse KL divergence\n","$$\\theta^* = \\arg\\min_\\theta D_{KL}(q_\\theta(x) \\parallel p(x))$$"]},{"cell_type":"code","metadata":{"id":"5-Rqv-1LSMBP","colab_type":"code","colab":{}},"source":["# Define your model here!\n","q_mean = None\n","q_cov = None\n","q = tfp.distributions.MultivariateNormalFullCovariance(q_mean, q_cov)\n","\n","# Define the loss here!\n","loss = None\n","train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n","sess.run(tf.global_variables_initializer())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lIlO1XDkSMBQ","colab_type":"text"},"source":["Lets run it for 20 iterations and visualize $q_{\\theta^{(t)}}(x)$ "]},{"cell_type":"code","metadata":{"scrolled":false,"id":"nUkK0TVgSMBR","colab_type":"code","colab":{}},"source":["fig, axes = plt.subplots(5,4, figsize=[12, 15])\n","\n","for i in range(5):\n","    for j in range(4):\n","        scatter(sample, axis=axes[i][j])\n","        x, y, xy_prob = prob_map(q, lim, N)\n","        contour(x, y, xy_prob, lim, axis=axes[i][j])\n","        axes[i][j].axis(lim)\n","        axes[i][j].axis('off')\n","        axes[i][j].set_title('iter: {}'.format(i*5 + j))\n","        sess.run(train)\n","        \n","plt.show()\n","plt.close(fig)\n","\n","sess.close()"],"execution_count":0,"outputs":[]}]}