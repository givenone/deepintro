{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"VAE.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"W8B8S9yHqAOF","colab_type":"text"},"source":["# VAE\n","Variational autoencoder [1] models inherit autoencoder architecture, but  use variational approach for latent representation learning. In this homework, we will implement VAE and quantitatively measure the quality of the generated samples via Inception score [2,3].\n","\n","[1] Auto-Encoding Variational Bayes, Diederik P Kingma, Max Welling 2013\n","https://arxiv.org/abs/1312.6114\n","\n","[2] Improved techniques for training gans, Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Rad- ford, A., and Chen, X. 2016\n","In Advances in Neural Information Processing Systems \n","\n","[3] A note on inception score, Shane Barratt, Rishi Sharma 2018\n","https://arxiv.org/abs/1801.01973\n"]},{"cell_type":"markdown","metadata":{"id":"v2Cy05g40Epk","colab_type":"text"},"source":["# PART I. Train a good VAE model"]},{"cell_type":"markdown","metadata":{"id":"3FRksiBgqlAh","colab_type":"text"},"source":["## Setup"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ooy_uAZNvHWT","colab":{}},"source":["import tensorflow as tf\n","if tf.__version__ < '2.0.0':         \n","    tf.enable_eager_execution()      \n","tf.executing_eagerly()               \n","\n","import numpy as np\n","import os\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# A bunch of utility functions\n","\n","def show_images(images):\n","    # images reshape to (batch_size, D)\n","    images = np.reshape(images, [images.shape[0], -1])\n","    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n","    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n","\n","    fig = plt.figure(figsize=(sqrtn, sqrtn))\n","    gs = gridspec.GridSpec(sqrtn, sqrtn)\n","    gs.update(wspace=0.05, hspace=0.05)\n","\n","    for i, img in enumerate(images):\n","        ax = plt.subplot(gs[i])\n","        plt.axis('off')\n","        ax.set_xticklabels([])\n","        ax.set_yticklabels([])\n","        ax.set_aspect('equal')\n","        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n","    return\n","\n","def preprocess_img(x):\n","    return 2 * x - 1.0\n","\n","def rel_error(x,y):\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","\n","def count_params(model):\n","    \"\"\"Count the number of parameters in the current TensorFlow graph \"\"\"\n","    param_count = np.sum([np.prod(p.shape) for p in model.weights])\n","    return param_count"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4crSgXuSrJ2L","colab_type":"text"},"source":["## Dataset\n","We will be working on the MNIST dataset, which is 60,000 training and 10,000 test images. Each picture contains a centered image of white digit on black background (0 through 9). This was one of the first datasets used to train convolutional neural networks and it is fairly easy -- a standard CNN model can easily exceed 99% accuracy. \n"," \n","\n","**Heads-up**: Our MNIST wrapper returns images as vectors. That is, they're size (batch, 784). If you want to treat them as images, we have to resize them to (batch,28,28) or (batch,28,28,1). They are also type np.float32 and bounded [0,1]. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mVyQDutTvZ2-","colab":{}},"source":["class MNIST(object):\n","    def __init__(self, batch_size, shuffle=False):\n","        \"\"\"\n","        Construct an iterator object over the MNIST data\n","        \n","        Inputs:\n","        - batch_size: Integer giving number of elements per minibatch\n","        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n","        \"\"\"\n","        train, _ = tf.keras.datasets.mnist.load_data()\n","        X, y = train\n","        X = X.astype(np.float32)/255\n","        X = X.reshape((X.shape[0], -1))\n","        self.X, self.y = X, y\n","        self.batch_size, self.shuffle = batch_size, shuffle\n","\n","    def __iter__(self):\n","        N, B = self.X.shape[0], self.batch_size\n","        idxs = np.arange(N)\n","        if self.shuffle:\n","            np.random.shuffle(idxs)\n","        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B)) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U35atJNUvlY6","colab":{}},"source":["# show a batch\n","mnist = MNIST(batch_size=16) \n","show_images(mnist.X[:16])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YFs-HgHH-Edo","colab":{}},"source":["X_DIM = mnist.X[0].size\n","num_samples = 100000\n","num_to_show = 100\n","\n","# Hyperparamters. Your job to find these.\n","# TODO:\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","num_epochs = None\n","batch_size = None\n","Z_DIM = None\n","learning_rate = None\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qe4AkjK9vlfz"},"source":["## Encoder\n","Our first step is to build a variational encoder network $q_\\phi(z \\mid x)$. \n","\n","**Hint:** You should use the layers in `tf.keras.layers` to build the model. Use four FC layers. All fully connected layers should include bias terms. For initialization, just use the default initializer used by the `tf.keras.layers` functions. \n","\n","The output of the encoder should thus have shape `[batch_size, 2*z_dim]`, and contain real numbers corresponding to the mean $\\mu(x_i)$ and diagonal log variance $\\log \\sigma(x_i)^2$ of each of the `batch_size` input images. Note, we want to make it return log of the variance for numerical stability.\n","\n","**WARNING:** Do not apply any non-linearity to the last activation."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bahWhGs5wt3W","colab":{}},"source":["def q_phi(z_dim=Z_DIM, x_dim=X_DIM):\n","  model = tf.keras.models.Sequential([\n","    # TODO: implement architecture\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****                                  \n","    \n","    pass\n","    \n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  ])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QHoTlHSK1Xt2","colab":{}},"source":["# TODO: implement reparameterization trick\n","def sample_z(mu, log_var):\n","  # Your code here for the reparameterization trick.\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  samples = None\n","\n","  pass\n","\n","  return samples\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SMEOgw0x1bpf"},"source":["## Decoder\n","Now to build a decoder network $p_\\theta(x \\mid z)$. You should use the layers in `tf.keras.layers` to construct the model. Use four FC layers. All fully connected layers should include bias terms. Note that you can use the tf.nn module to access activation functions. Once again, use the default initializers for parameters.\n","\n","In this exercise, we will use Bernoulli MLP decoder where $p_\\theta(x \\mid z)$ is modeled with multivariate Bernoulli distribution, in contrast to the Gaussian distribution we discussed in the lecture, as following (see Appendix C.1 in the original paper for more details).\n","\n","$\\log p(x \\mid z) = \\sum_{i=1} x_i \\log z_i + (1-x_i) \\log (1-z_i)$\n","\n","Note, the output of the decoder should have shape `[batch_size, x_dim]` and should output the unnormalized logits of $x_i$.\n","\n","**WARNING:** Do not apply any non-linearity to the last activation."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4RXB7XOl1g72","colab":{}},"source":["def p_theta(z_dim=Z_DIM, x_dim=X_DIM):\n","  model = tf.keras.models.Sequential([\n","    # TODO: implement architecture\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    \n","    pass\n","    \n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  ])\n","  return model                             "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7xOZpzso2w3Y"},"source":["## Loss definition\n","Compute the VAE loss. \n","1. For the reconstruction loss, you might find `tf.nn.sigmoid_cross_entropy_with_logits` or `tf.keras.losses.BinaryCrossentropy` useful.\n","2. For the kl loss, we discussed the closed form kl divergence between two gaussians in the lecture."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"40qNrPkT3D8s","colab":{}},"source":["def vae_loss(x, x_logit, z_mu, z_logvar):\n","  recon_loss = None\n","  kl_loss = None\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  \n","  pass\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n","  return vae_loss, tf.reduce_mean(recon_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJgFsfCDzJWu","colab_type":"text"},"source":["## Optimizing our loss\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P27H8vbu54Ds","colab":{}},"source":["Q = q_phi()\n","P = p_theta()\n","solver = tf.keras.optimizers.Adam(learning_rate)      \n","mnist = MNIST(batch_size=batch_size, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qTbSDHNLcNXd"},"source":["Visualize generated samples before training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xxMtTL6IcICm","colab":{}},"source":["z_gen = tf.random.normal(shape=[num_to_show, Z_DIM])\n","x_gen = P(z_gen)\n","imgs_numpy = tf.nn.sigmoid(x_gen).numpy()\n","show_images(imgs_numpy)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6xVN3sBzWD3","colab_type":"text"},"source":["## Training a VAE!\n","If everything works, your batch average reconstruction loss should drop below 95."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zbn5KW9T-4BF","colab":{}},"source":["iter_count = 0\n","show_every = 200\n","for epoch in range(num_epochs):\n","  for (x_i, _) in mnist:\n","    with tf.GradientTape() as tape:\n","      z_concat = Q(preprocess_img(x_i))\n","      z_mu, z_logvar = tf.split(z_concat, num_or_size_splits=2, axis=1)\n","      z_i = sample_z(z_mu, z_logvar)\n","\n","      x_logit = P(z_i)\n","      \n","      loss, recon_loss = vae_loss(x_i, x_logit, z_mu, z_logvar)\n","\n","      grads = tape.gradient(loss, \n","                [Q.trainable_variables, P.trainable_variables])\n","\n","      solver.apply_gradients(zip([*grads[0],*grads[1]],              \n","                [*Q.trainable_variables, *P.trainable_variables]))   \n","\n","      if (iter_count % show_every == 0):\n","        print('Epoch: {}, Iter: {}, Loss: {:.4}, Recon: {:.4}'.format(\n","            epoch, iter_count, loss, recon_loss))\n","        #imgs_numpy = tf.nn.sigmoid(x_logit).numpy()\n","        #show_images(imgs_numpy[0:16])\n","        #plt.show()\n","      iter_count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IJR3cerpcSPu"},"source":["Visualize generated samples after training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"duZZIrDGbmkb","colab":{}},"source":["z_gen = tf.random.normal(shape=[num_to_show, Z_DIM])\n","x_gen = P(z_gen)\n","imgs_numpy = tf.nn.sigmoid(x_gen).numpy()\n","show_images(imgs_numpy)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ybqA40bFzzd0","colab_type":"text"},"source":["# PART II. Compute the inception score for your trained VAE model\n","In this part, we will quantitavely measure how good your VAE model is."]},{"cell_type":"markdown","metadata":{"id":"gjSxHv5JOcfd","colab_type":"text"},"source":["### Train a classifier\n","We first need to train a classifier. "]},{"cell_type":"code","metadata":{"id":"5mNNF_tJJzj0","colab_type":"code","colab":{}},"source":["batch_size = 128\n","num_classes = 10\n","epochs = 20\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","x_train = x_train.reshape(60000, 784)\n","x_test = x_test.reshape(10000, 784)\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n","\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)))\n","model.add(tf.keras.layers.Dropout(0.2))\n","model.add(tf.keras.layers.Dense(512, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.2))\n","model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=tf.keras.optimizers.RMSprop(),\n","              metrics=['accuracy'])\n","\n","history = model.fit(x_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3i4W8U3u0mCZ","colab_type":"text"},"source":["###Verify the trained classifier on the generated samples\n","Generate samples and visually inspect if the predicted labels on the samples match the actual digits in generated images."]},{"cell_type":"code","metadata":{"id":"CLh6RRuL0vAi","colab_type":"code","colab":{}},"source":["z_gen = tf.random.normal(shape=[num_samples, Z_DIM])\n","x_gen = P(z_gen)\n","imgs_numpy = tf.nn.sigmoid(x_gen[:num_to_show]).numpy()\n","show_images(imgs_numpy)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSecn8Mqc4kz","colab_type":"code","colab":{}},"source":["np.argmax(model.predict(tf.nn.sigmoid(x_gen[:20])), axis=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6OMwL6FA1jX6","colab_type":"text"},"source":["### Implement the inception score\n","Implement Equation 1 in the reference [3]. Replace expectation in the equation with empirical average of `num_samples` samples. Don't forget the exponentiation at the end. You should get Inception score of at least 9.0."]},{"cell_type":"code","metadata":{"id":"T90AT5V8Mold","colab_type":"code","colab":{}},"source":["kld_obj = tf.keras.losses.KLDivergence()\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","score = None\n","pass\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","print('Inception score: {:.4}'.format(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a0r7aK4v2FwE","colab_type":"text"},"source":["### Plot the histogram of predicted labels\n","Let's additionally inspect the class diversity of the generated samples."]},{"cell_type":"code","metadata":{"id":"woxFDpsnb2ZD","colab_type":"code","colab":{}},"source":["plt.hist(np.argmax(model.predict(tf.nn.sigmoid(x_gen)), axis=-1),\n","         bins=np.arange(11)-0.5, rwidth=0.8, density=True)\n","plt.xticks(range(10))\n","plt.show()"],"execution_count":0,"outputs":[]}]}